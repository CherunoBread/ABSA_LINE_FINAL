{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d00A7BYIOveF",
        "outputId": "e1926258-9383-4994-ec31-d63a203feb02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.36.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2026.1.4)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.19.5 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (0.14.1)\n",
            "Requirement already satisfied: numpy<3,>=1.25.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (2.0.2)\n",
            "Requirement already satisfied: scipy<2,>=1.11.4 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.16.3)\n",
            "Requirement already satisfied: scikit-learn<2,>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.6.1)\n",
            "Requirement already satisfied: sklearn-compat<0.2,>=0.1.5 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (0.1.5)\n",
            "Requirement already satisfied: joblib<2,>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (1.5.3)\n",
            "Requirement already satisfied: threadpoolctl<4,>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from imbalanced-learn) (3.6.0)\n",
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.15.0\n",
            "Collecting google-play-scraper\n",
            "  Downloading google_play_scraper-1.2.7-py3-none-any.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading google_play_scraper-1.2.7-py3-none-any.whl (28 kB)\n",
            "Installing collected packages: google-play-scraper\n",
            "Successfully installed google-play-scraper-1.2.7\n",
            "Collecting sastrawi\n",
            "  Downloading Sastrawi-1.0.1-py2.py3-none-any.whl.metadata (909 bytes)\n",
            "Downloading Sastrawi-1.0.1-py2.py3-none-any.whl (209 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m209.7/209.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sastrawi\n",
            "Successfully installed sastrawi-1.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers\n",
        "!pip install pandas\n",
        "!pip install scikit-learn\n",
        "!pip install imbalanced-learn\n",
        "!pip install emoji\n",
        "!pip install google-play-scraper\n",
        "!pip install sastrawi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IsNeUYkwPhRn",
        "outputId": "845c9e72-a30e-47ee-8591-82df41891cd9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data shape: (5323, 7)\n",
            "       userName                                            content  score  \\\n",
            "0      æ—‹å¾‹melody                                                ðŸ‘ðŸ‘ðŸ‘      5   \n",
            "1  Fathur Rohim                             Bagus untuk komunikasi      5   \n",
            "2     May Syifa  halo, maaf kenapa fitur tema di line saya ngga...      2   \n",
            "3       A L E X  GABISA LOGIN LAGI WOY ðŸ˜’ AKUN ADA , LOGIN PAKE ...      1   \n",
            "4   Alfi Ismail  tolong lah line nya di perbagus lagi kaya wa ....      1   \n",
            "\n",
            "                 at Topic_1_Pengalaman_Umum_Penggunaan_LINE  \\\n",
            "0   12/21/2025 8:02                                 Positif   \n",
            "1   12/21/2025 2:54                                 Positif   \n",
            "2   12/21/2025 1:10                                  Netral   \n",
            "3  12/20/2025 17:36                                  Netral   \n",
            "4  12/20/2025 16:49                                  Netral   \n",
            "\n",
            "  Topic_2_Fitur_Tambahan Topic_3_Login_dan_Registrasi_Akun  \n",
            "0                 Netral                            Netral  \n",
            "1                 Netral                            Netral  \n",
            "2                Negatif                            Netral  \n",
            "3                 Netral                           Negatif  \n",
            "4                 Netral                           Negatif  \n",
            "\n",
            "Distribusi Topic 1 (Pengalaman Umum Penggunaan LINE):\n",
            "Topic_1_Pengalaman_Umum_Penggunaan_LINE\n",
            "Netral     2961\n",
            "Negatif    1553\n",
            "Positif     809\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribusi Topic 2 (Fitur Tambahan):\n",
            "Topic_2_Fitur_Tambahan\n",
            "Netral     4052\n",
            "Negatif    1251\n",
            "Positif      20\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Distribusi Topic 3 (Login dan Registrasi Akun):\n",
            "Topic_3_Login_dan_Registrasi_Akun\n",
            "Netral     3326\n",
            "Negatif    1990\n",
            "Positif       7\n",
            "Name: count, dtype: int64\n",
            "Sedang melakukan normalisasi, stopword removal, dan stemming... (Mohon tunggu sebentar)\n",
            "Sebelum drop: (5323, 8)\n",
            "Sesudah drop: (5323, 8)\n",
            "\n",
            "Unique values Topic 1:\n",
            "array(['Positif', 'Netral', 'Negatif'], dtype=object)\n",
            "\n",
            "Unique values Topic 2:\n",
            "array(['Netral', 'Negatif', 'Positif'], dtype=object)\n",
            "\n",
            "Unique values Topic 3:\n",
            "array(['Netral', 'Negatif', 'Positif'], dtype=object)\n",
            "\n",
            "Mapping Topic 1 (Pengalaman Umum Penggunaan LINE):\n",
            "{'Negatif': np.int64(0), 'Netral': np.int64(1), 'Positif': np.int64(2)}\n",
            "\n",
            "Mapping Topic 2 (Fitur Tambahan):\n",
            "{'Negatif': np.int64(0), 'Netral': np.int64(1), 'Positif': np.int64(2)}\n",
            "\n",
            "Mapping Topic 3 (Login dan Registrasi Akun):\n",
            "{'Negatif': np.int64(0), 'Netral': np.int64(1), 'Positif': np.int64(2)}\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import emoji\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "\n",
        "# Load data\n",
        "df = pd.read_csv(\"line_reviews_labeled_with_confidence.csv\")\n",
        "print(\"Data shape:\", df.shape)\n",
        "print(df.head())\n",
        "\n",
        "# Tampilkan value counts untuk setiap topik\n",
        "print(\"\\nDistribusi Topic 1 (Pengalaman Umum Penggunaan LINE):\")\n",
        "print(df['Topic_1_Pengalaman_Umum_Penggunaan_LINE'].value_counts())\n",
        "print(\"\\nDistribusi Topic 2 (Fitur Tambahan):\")\n",
        "print(df['Topic_2_Fitur_Tambahan'].value_counts())\n",
        "print(\"\\nDistribusi Topic 3 (Login dan Registrasi Akun):\")\n",
        "print(df['Topic_3_Login_dan_Registrasi_Akun'].value_counts())\n",
        "\n",
        "def clean_text(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    # 1. lowercase\n",
        "    text = text.lower()\n",
        "    # 2. hapus URL\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    # 3. hapus emoji\n",
        "    all_chars = list(text)\n",
        "    text = \"\".join([c for c in all_chars if not emoji.is_emoji(c)])\n",
        "    # 4. hapus angka\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    # 5. hapus karakter non-alfabet\n",
        "    text = re.sub(r'[^a-zA-Z0-9\\s]', '', text)\n",
        "    # 6. hapus spasi ganda\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    return text\n",
        "\n",
        "norm_dict = {\n",
        "    \"gak\": \"tidak\", \"ga\": \"tidak\", \"gk\": \"tidak\", \"tdk\": \"tidak\", \"enggak\": \"tidak\",\n",
        "    \"nggak\": \"tidak\", \"kagak\": \"tidak\",\n",
        "    \"gw\": \"saya\", \"gua\": \"saya\", \"aku\": \"saya\", \"sy\": \"saya\", \"gue\": \"saya\",\n",
        "    \"lu\": \"kamu\", \"lo\": \"kamu\", \"km\": \"kamu\", \"kalian\": \"kamu\",\n",
        "    \"yg\": \"yang\", \"kalo\": \"kalau\", \"klo\": \"kalau\",\n",
        "    \"krn\": \"karena\", \"karna\": \"karena\",\n",
        "    \"utk\": \"untuk\", \"untk\": \"untuk\",\n",
        "    \"dgn\": \"dengan\", \"dlm\": \"dalam\",\n",
        "    \"sdh\": \"sudah\", \"udh\": \"sudah\", \"udah\": \"sudah\",\n",
        "    \"blm\": \"belum\",\n",
        "    \"jg\": \"juga\", \"jga\": \"juga\",\n",
        "    \"tp\": \"tetapi\", \"tapi\": \"tetapi\",\n",
        "    \"aja\": \"saja\", \"aj\": \"saja\",\n",
        "    \"bgt\": \"sangat\", \"banget\": \"sangat\",\n",
        "    \"knp\": \"kenapa\", \"napa\": \"kenapa\",\n",
        "    \"gmn\": \"bagaimana\", \"gimana\": \"bagaimana\",\n",
        "    \"bs\": \"bisa\", \"bisaa\": \"bisa\", \"gabisa\": \"tidak bisa\", \"ga bisa\": \"tidak bisa\",\n",
        "    \"trus\": \"terus\", \"trs\": \"terus\",\n",
        "    \"jd\": \"jadi\", \"jdi\": \"jadi\",\n",
        "    \"pdhl\": \"padahal\",\n",
        "    \"bnyk\": \"banyak\",\n",
        "    \"sm\": \"sama\",\n",
        "    \"lbh\": \"lebih\",\n",
        "    \"dr\": \"dari\",\n",
        "    \"eror\": \"error\", \"erorr\": \"error\", \"errorr\": \"error\",\n",
        "    \"apk\": \"aplikasi\", \"apps\": \"aplikasi\", \"aplikasine\": \"aplikasi\",\n",
        "    \"hp\": \"ponsel\", \"handphone\": \"ponsel\",\n",
        "    \"download\": \"unduh\", \"donlot\": \"unduh\",\n",
        "    \"update\": \"pembaruan\", \"updet\": \"pembaruan\",\n",
        "    \"login\": \"masuk\", \"log in\": \"masuk\", \"sign in\": \"masuk\",\n",
        "    \"log out\": \"keluar\",\n",
        "    \"notif\": \"notifikasi\",\n",
        "    \"verif\": \"verifikasi\",\n",
        "    \"no\": \"nomor\", \"nmr\": \"nomor\",\n",
        "    \"pw\": \"kata sandi\", \"password\": \"kata sandi\", \"sandi\": \"kata sandi\",\n",
        "    \"chat\": \"pesan\", \"chatting\": \"pesan\",\n",
        "    \"call\": \"panggilan\", \"nelpon\": \"panggilan\", \"telfon\": \"panggilan\",\n",
        "    \"voom\": \"timeline\", \"line voom\": \"timeline\",\n",
        "    \"tolong\": \"mohon\", \"pls\": \"mohon\", \"please\": \"mohon\", \"plis\": \"mohon\",\n",
        "    \"balikin\": \"kembalikan\",\n",
        "    \"ilang\": \"hilang\",\n",
        "    \"tau\": \"tahu\",\n",
        "    \"liat\": \"lihat\",\n",
        "    \"cuman\": \"hanya\", \"cuma\": \"hanya\",\n",
        "    \"makasih\": \"terima kasih\", \"tq\": \"terima kasih\",\n",
        "    \"mulu\": \"terus\", \"melulu\": \"terus\",\n",
        "    \"bener\": \"benar\",\n",
        "    \"sampe\": \"sampai\",\n",
        "    \"kapan\": \"kapan\",\n",
        "    \"kocakk\": \"kocak\"\n",
        "}\n",
        "\n",
        "custom_stopwords = {\n",
        "    \"dan\", \"atau\", \"juga\",\n",
        "    \"yang\", \"di\", \"ke\", \"dari\", \"pada\", \"dalam\", \"untuk\", \"bagi\", \"kepada\", \"oleh\",\n",
        "    \"ini\", \"itu\", \"tersebut\", \"disini\", \"disitu\",\n",
        "    \"saya\", \"aku\", \"kamu\", \"dia\", \"mereka\", \"kita\", \"kami\", \"anda\", \"kalian\",\n",
        "    \"bisa\", \"dapat\", \"akan\", \"sedang\", \"sudah\", \"telah\", \"masih\", \"belum\",\n",
        "    \"ada\", \"adalah\", \"ialah\", \"merupakan\", \"sebagai\", \"seperti\",\n",
        "    \"sih\", \"dong\", \"deh\", \"kok\", \"lah\", \"mah\", \"kan\", \"pun\", \"doang\",\n",
        "    \"nya\",\n",
        "    \"saja\", \"aja\",\n",
        "    \"padahal\", \"walaupun\", \"meskipun\",\n",
        "    \"karena\", \"sebab\", \"akibat\", \"sehingga\", \"maka\",\n",
        "    \"terus\", \"lalu\", \"kemudian\", \"akhirnya\",\n",
        "    \"mohon\", \"tolong\", \"harap\", \"silakan\",\n",
        "    \"terima\", \"kasih\", \"makasih\",\n",
        "    \"tanya\", \"tahu\", \"kasih\", \"banyak\", \"sedikit\", \"kurang\", \"lebih\",\n",
        "    \"hari\", \"tanggal\", \"bulan\", \"tahun\", \"jam\", \"waktu\",\n",
        "    \"kali\", \"tiap\", \"setiap\",\n",
        "    \"apa\", \"kenapa\", \"bagaimana\", \"dimana\", \"siapa\",\n",
        "    \"halo\", \"hai\", \"min\", \"admin\", \"kak\", \"gan\", \"sis\", \"bro\"\n",
        "}\n",
        "\n",
        "factory = StemmerFactory()\n",
        "stemmer = factory.create_stemmer()\n",
        "\n",
        "# Terapkan cleaning ke kolom 'content'\n",
        "df['content_clean'] = df['content'].apply(clean_text)\n",
        "\n",
        "def advanced_preprocessing(text: str) -> str:\n",
        "    if not isinstance(text, str) or not text:\n",
        "        return \"\"\n",
        "\n",
        "    words = text.split()\n",
        "\n",
        "    # Tahap A: Normalisasi Kata (Slang -> Baku)\n",
        "    words = [norm_dict.get(w, w) for w in words]\n",
        "\n",
        "    # Tahap B: Stopword Removal (Hapus kata umum yang tidak penting)\n",
        "    # Kita lakukan sebelum stemming agar proses stemming lebih cepat\n",
        "    words = [w for w in words if w not in custom_stopwords]\n",
        "\n",
        "    # Tahap C: Stemming (Mengembalikan ke kata dasar)\n",
        "    # Gabung dulu jadi string karena Sastrawi memproses kalimat\n",
        "    text_processed = \" \".join(words)\n",
        "    text_stemmed = stemmer.stem(text_processed)\n",
        "\n",
        "    return text_stemmed\n",
        "\n",
        "# --- 4. TERAPKAN KE DATAFRAME ---\n",
        "# Kita gunakan kolom 'content_clean' yang sudah Anda buat sebelumnya sebagai input\n",
        "print(\"Sedang melakukan normalisasi, stopword removal, dan stemming... (Mohon tunggu sebentar)\")\n",
        "df['content_clean'] = df['content_clean'].apply(advanced_preprocessing)\n",
        "\n",
        "print(\"Sebelum drop:\", df.shape)\n",
        "# Drop baris yang memiliki nilai null di kolom penting\n",
        "df = df.dropna(subset=['content_clean', 'Topic_1_Pengalaman_Umum_Penggunaan_LINE',\n",
        "                       'Topic_2_Fitur_Tambahan', 'Topic_3_Login_dan_Registrasi_Akun'])\n",
        "print(\"Sesudah drop:\", df.shape)\n",
        "\n",
        "# Tampilkan daftar unik untuk setiap topik\n",
        "print(\"\\nUnique values Topic 1:\")\n",
        "print(repr(df['Topic_1_Pengalaman_Umum_Penggunaan_LINE'].unique()))\n",
        "print(\"\\nUnique values Topic 2:\")\n",
        "print(repr(df['Topic_2_Fitur_Tambahan'].unique()))\n",
        "print(\"\\nUnique values Topic 3:\")\n",
        "print(repr(df['Topic_3_Login_dan_Registrasi_Akun'].unique()))\n",
        "\n",
        "# Label encoding untuk setiap topik\n",
        "le_topic1 = LabelEncoder()\n",
        "le_topic2 = LabelEncoder()\n",
        "le_topic3 = LabelEncoder()\n",
        "\n",
        "df['label_topic1'] = le_topic1.fit_transform(df['Topic_1_Pengalaman_Umum_Penggunaan_LINE'])\n",
        "df['label_topic2'] = le_topic2.fit_transform(df['Topic_2_Fitur_Tambahan'])\n",
        "df['label_topic3'] = le_topic3.fit_transform(df['Topic_3_Login_dan_Registrasi_Akun'])\n",
        "\n",
        "# Tampilkan mapping untuk setiap topik\n",
        "print(\"\\nMapping Topic 1 (Pengalaman Umum Penggunaan LINE):\")\n",
        "print(dict(zip(le_topic1.classes_, le_topic1.transform(le_topic1.classes_))))\n",
        "print(\"\\nMapping Topic 2 (Fitur Tambahan):\")\n",
        "print(dict(zip(le_topic2.classes_, le_topic2.transform(le_topic2.classes_))))\n",
        "print(\"\\nMapping Topic 3 (Login dan Registrasi Akun):\")\n",
        "print(dict(zip(le_topic3.classes_, le_topic3.transform(le_topic3.classes_))))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9eJExzMJ2aNu",
        "outputId": "1fcfa8da-185b-4078-9c34-9bba45f0dc2f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total data: 5323\n",
            "Distribusi Topic 1: {1: 2961, 0: 1553, 2: 809}\n",
            "Distribusi Topic 2: {1: 4052, 0: 1251, 2: 20}\n",
            "Distribusi Topic 3: {1: 3326, 0: 1990, 2: 7}\n",
            "\n",
            "=== TOPIC 1 SPLIT ===\n",
            "Topic 1 - Original distribution: {1: 2961, 0: 1553, 2: 809}\n",
            "Topic 1 - Train: 3726, Val: 532, Test: 1065\n",
            "Topic 1 - Train labels: {1: 2073, 0: 1087, 2: 566}\n",
            "Topic 1 - Val labels: {1: 296, 0: 155, 2: 81}\n",
            "Topic 1 - Test labels: {1: 592, 0: 311, 2: 162}\n",
            "\n",
            "=== TOPIC 2 SPLIT ===\n",
            "Topic 2 - Original distribution: {1: 4052, 0: 1251, 2: 20}\n",
            "Topic 2 - Train: 3726, Val: 532, Test: 1065\n",
            "Topic 2 - Train labels: {1: 2836, 0: 876, 2: 14}\n",
            "Topic 2 - Val labels: {1: 405, 0: 125, 2: 2}\n",
            "Topic 2 - Test labels: {1: 811, 0: 250, 2: 4}\n",
            "\n",
            "=== TOPIC 3 SPLIT ===\n",
            "Topic 3 - Original distribution: {1: 3326, 0: 1990, 2: 7}\n",
            "Topic 3 - Train: 3726, Val: 532, Test: 1065\n",
            "Topic 3 - Train labels: {1: 2328, 0: 1393, 2: 5}\n",
            "Topic 3 - Val labels: {1: 332, 0: 199, 2: 1}\n",
            "Topic 3 - Test labels: {1: 666, 0: 398, 2: 1}\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "import pandas as pd\n",
        "\n",
        "# 1. Ekstrak teks dan label untuk setiap topik\n",
        "print(\"Total data:\", len(df))\n",
        "print(\"Distribusi Topic 1:\", pd.Series(df['label_topic1']).value_counts().to_dict())\n",
        "print(\"Distribusi Topic 2:\", pd.Series(df['label_topic2']).value_counts().to_dict())\n",
        "print(\"Distribusi Topic 3:\", pd.Series(df['label_topic3']).value_counts().to_dict())\n",
        "\n",
        "# 2. Split 7:1:2 per aspek\n",
        "splits = {}\n",
        "for i in [1, 2, 3]:\n",
        "    texts = df['content_clean'].tolist()\n",
        "    labels = df[f'label_topic{i}'].tolist()\n",
        "\n",
        "    print(f\"\\n=== TOPIC {i} SPLIT ===\")\n",
        "    print(f\"Topic {i} - Original distribution: {pd.Series(labels).value_counts().to_dict()}\")\n",
        "\n",
        "    # Cek apakah bisa stratify (minimal 2 sampel per kelas)\n",
        "    label_counts = pd.Series(labels).value_counts()\n",
        "    can_stratify_first = all(count >= 2 for count in label_counts.values)\n",
        "\n",
        "    # 70% train / 30% temp\n",
        "    if can_stratify_first:\n",
        "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "            texts, labels,\n",
        "            test_size=0.30,\n",
        "            random_state=42,\n",
        "            stratify=labels\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Topic {i}: Cannot stratify first split (some classes < 2 samples)\")\n",
        "        X_train, X_temp, y_train, y_temp = train_test_split(\n",
        "            texts, labels,\n",
        "            test_size=0.30,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    # Cek apakah bisa stratify untuk split kedua\n",
        "    temp_label_counts = pd.Series(y_temp).value_counts()\n",
        "    can_stratify_second = all(count >= 2 for count in temp_label_counts.values)\n",
        "\n",
        "    # dari temp (30%): 1/3 â†’ val (10%), 2/3 â†’ test (20%)\n",
        "    if can_stratify_second:\n",
        "        X_val, X_test, y_val, y_test = train_test_split(\n",
        "            X_temp, y_temp,\n",
        "            test_size=2/3,\n",
        "            random_state=42,\n",
        "            stratify=y_temp\n",
        "        )\n",
        "    else:\n",
        "        print(f\"Topic {i}: Cannot stratify second split (some classes < 2 samples in temp)\")\n",
        "        X_val, X_test, y_val, y_test = train_test_split(\n",
        "            X_temp, y_temp,\n",
        "            test_size=2/3,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "    splits[f't{i}'] = {\n",
        "        'X_train': X_train, 'y_train': y_train,\n",
        "        'X_val': X_val, 'y_val': y_val,\n",
        "        'X_test': X_test, 'y_test': y_test,\n",
        "    }\n",
        "\n",
        "    print(f\"Topic {i} - Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")\n",
        "    print(f\"Topic {i} - Train labels: {pd.Series(y_train).value_counts().to_dict()}\")\n",
        "    print(f\"Topic {i} - Val labels: {pd.Series(y_val).value_counts().to_dict()}\")\n",
        "    print(f\"Topic {i} - Test labels: {pd.Series(y_test).value_counts().to_dict()}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import re\n",
        "import emoji\n",
        "from Sastrawi.Stemmer.StemmerFactory import StemmerFactory\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
        "import numpy as np\n",
        "from transformers import AutoTokenizer, BertTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset\n",
        "import torch\n",
        "\n",
        "# Load data (assuming df is already loaded and preprocessed up to this point)\n",
        "# df = pd.read_csv(\"line_reviews_labeled_with_confidence.csv\") # This part was already done\n",
        "\n",
        "# Assuming tokenizer is already loaded\n",
        "tokenizer = BertTokenizer.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "model = AutoModel.from_pretrained(\"indobenchmark/indobert-base-p1\")\n",
        "\n",
        "class IndobertDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "        enc = self.tokenizer.encode_plus(\n",
        "            text,\n",
        "            add_special_tokens=True,\n",
        "            max_length=self.max_len,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            return_attention_mask=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        return {\n",
        "            'input_ids': enc['input_ids'].flatten(),\n",
        "            'attention_mask': enc['attention_mask'].flatten(),\n",
        "            'labels': torch.tensor(label, dtype=torch.long)\n",
        "        }\n",
        "\n",
        "    def show_tokenization(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        print(f\"Original Text (from dataset): {text}\")\n",
        "        print(f\"Original Label (from dataset): {label}\")\n",
        "\n",
        "        item = self.__getitem__(idx)\n",
        "        input_ids = item['input_ids']\n",
        "\n",
        "        tokens = self.tokenizer.convert_ids_to_tokens(input_ids)\n",
        "        decoded_text = self.tokenizer.decode(input_ids, skip_special_tokens=False)\n",
        "\n",
        "        print(f\"Tokenized (list of tokens): {tokens}\")\n",
        "        print(f\"Tokenized and Decoded (using tokenizer.decode): {decoded_text}\")\n",
        "        print(f\"Input IDs: {input_ids.tolist()}\")\n",
        "        # No return value needed here as it's primarily for printing\n",
        "\n",
        "\n",
        "# 4. Instantiate dataset untuk tiap aspek\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"MEMBUAT DATASET UNTUK SETIAP TOPIK\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Topic 1 datasets\n",
        "train_dataset_t1 = IndobertDataset(texts=splits['t1']['X_train'],\n",
        "                                   labels=splits['t1']['y_train'],\n",
        "                                   tokenizer=tokenizer)\n",
        "val_dataset_t1 = IndobertDataset(texts=splits['t1']['X_val'],\n",
        "                                 labels=splits['t1']['y_val'],\n",
        "                                 tokenizer=tokenizer)\n",
        "test_dataset_t1 = IndobertDataset(texts=splits['t1']['X_test'],\n",
        "                                  labels=splits['t1']['y_test'],\n",
        "                                  tokenizer=tokenizer)\n",
        "\n",
        "# Topic 2 datasets\n",
        "train_dataset_t2 = IndobertDataset(splits['t2']['X_train'], splits['t2']['y_train'], tokenizer)\n",
        "val_dataset_t2 = IndobertDataset(splits['t2']['X_val'], splits['t2']['y_val'], tokenizer)\n",
        "test_dataset_t2 = IndobertDataset(splits['t2']['X_test'], splits['t2']['y_test'], tokenizer)\n",
        "\n",
        "# Topic 3 datasets\n",
        "train_dataset_t3 = IndobertDataset(splits['t3']['X_train'], splits['t3']['y_train'], tokenizer)\n",
        "val_dataset_t3 = IndobertDataset(splits['t3']['X_val'], splits['t3']['y_val'], tokenizer)\n",
        "test_dataset_t3 = IndobertDataset(splits['t3']['X_test'], splits['t3']['y_test'], tokenizer)\n",
        "\n",
        "print(f\"Dataset Topic 1 - Train: {len(train_dataset_t1)}, Val: {len(val_dataset_t1)}, Test: {len(test_dataset_t1)}\")\n",
        "print(f\"Dataset Topic 2 - Train: {len(train_dataset_t2)}, Val: {len(val_dataset_t2)}, Test: {len(test_dataset_t2)}\")\n",
        "print(f\"Dataset Topic 3 - Train: {len(train_dataset_t3)}, Val: {len(val_dataset_t3)}, Test: {len(test_dataset_t3)}\")\n",
        "\n",
        "# 5. Ringkasan untuk referensi\n",
        "datasets = {\n",
        "    'topic1': {\n",
        "        'train': train_dataset_t1,\n",
        "        'val': val_dataset_t1,\n",
        "        'test': test_dataset_t1\n",
        "    },\n",
        "    'topic2': {\n",
        "        'train': train_dataset_t2,\n",
        "        'val': val_dataset_t2,\n",
        "        'test': test_dataset_t2\n",
        "    },\n",
        "    'topic3': {\n",
        "        'train': train_dataset_t3,\n",
        "        'val': val_dataset_t3,\n",
        "        'test': test_dataset_t3\n",
        "    }\n",
        "}"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d81Hp9B14Bm",
        "outputId": "37c73984-7df3-43a9-ff04-7fd9133e1ff8"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "MEMBUAT DATASET UNTUK SETIAP TOPIK\n",
            "==================================================\n",
            "Dataset Topic 1 - Train: 3726, Val: 532, Test: 1065\n",
            "Dataset Topic 2 - Train: 3726, Val: 532, Test: 1065\n",
            "Dataset Topic 3 - Train: 3726, Val: 532, Test: 1065\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1be5acb",
        "outputId": "6eac9547-6dba-45cf-bd01-2be161963ed3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "============================================================\n",
            "VISUALISASI DATASET DARI HASIL PREPROCESSING (JOINED)\n",
            "============================================================\n",
            "Original Text (from dataset): kembali fitur timeline guna\n",
            "Original Label (from dataset): 0\n",
            "Tokenized (list of tokens): ['[CLS]', 'kembali', 'fitur', 'timeline', 'guna', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "Tokenized and Decoded (using tokenizer.decode): [CLS] kembali fitur timeline guna [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Input IDs: [2, 755, 2631, 29345, 3208, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "............................................................\n",
            "Original Text (from dataset): nyata respon customer servis bagus akun pulih moga tidak ulang kembali ban alas tidak jelas\n",
            "Original Label (from dataset): 0\n",
            "Tokenized (list of tokens): ['[CLS]', 'nyata', 'respon', 'customer', 'servis', 'bagus', 'akun', 'pulih', 'moga', 'tidak', 'ulang', 'kembali', 'ban', 'alas', 'tidak', 'jelas', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "Tokenized and Decoded (using tokenizer.decode): [CLS] nyata respon customer servis bagus akun pulih moga tidak ulang kembali ban alas tidak jelas [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Input IDs: [2, 3325, 5040, 6751, 10814, 1305, 2641, 14031, 16983, 119, 2420, 755, 207, 8705, 119, 1127, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "............................................................\n",
            "Original Text (from dataset): tidak daftar nomor telepon malah tulis nomor telepon tidak tambah negara temen tambah nomor telepon aneh\n",
            "Original Label (from dataset): 0\n",
            "Tokenized (list of tokens): ['[CLS]', 'tidak', 'daftar', 'nomor', 'telepon', 'malah', 'tulis', 'nomor', 'telepon', 'tidak', 'tambah', 'negara', 'temen', 'tambah', 'nomor', 'telepon', 'aneh', '[SEP]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]', '[PAD]']\n",
            "Tokenized and Decoded (using tokenizer.decode): [CLS] tidak daftar nomor telepon malah tulis nomor telepon tidak tambah negara temen tambah nomor telepon aneh [SEP] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\n",
            "Input IDs: [2, 119, 1109, 1288, 3178, 2592, 1533, 1288, 3178, 119, 3837, 664, 6141, 3837, 1288, 3178, 3526, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
            "\n",
            "............................................................\n"
          ]
        }
      ],
      "source": [
        "# 1. Data mentah (List of lists) dari kamu\n",
        "raw_tokens_data = [\n",
        "    ['kembali', 'fitur', 'timeline', 'guna'],\n",
        "\n",
        "    ['nyata', 'respon', 'customer', 'servis', 'bagus', 'akun', 'pulih',\n",
        "     'moga', 'tidak', 'ulang', 'kembali', 'ban', 'alas', 'tidak', 'jelas'],\n",
        "\n",
        "    ['tidak', 'daftar', 'nomor', 'telepon', 'malah', 'tulis', 'nomor',\n",
        "     'telepon', 'tidak', 'tambah', 'negara', 'temen', 'tambah', 'nomor', 'telepon', 'aneh']\n",
        "]\n",
        "\n",
        "# 2. PROSES PENGGABUNGAN (JOIN)\n",
        "# Mengubah ['kembali', 'fitur'] menjadi \"kembali fitur\"\n",
        "joined_texts = [\" \".join(tokens) for tokens in raw_tokens_data]\n",
        "\n",
        "# 3. Buat Label Dummy (sekadar syarat agar class Dataset jalan)\n",
        "dummy_labels = [0] * len(joined_texts)\n",
        "\n",
        "# 4. Masukkan ke IndobertDataset\n",
        "# Pastikan class IndobertDataset sudah memiliki method show_tokenization yang baru\n",
        "processed_dataset = IndobertDataset(\n",
        "    texts=joined_texts,\n",
        "    labels=dummy_labels,\n",
        "    tokenizer=tokenizer,\n",
        "    max_len=128\n",
        ")\n",
        "\n",
        "# 5. Tampilkan Hasil\n",
        "print(\"=\"*60)\n",
        "print(\"VISUALISASI DATASET DARI HASIL PREPROCESSING (JOINED)\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "for i in range(len(processed_dataset)):\n",
        "    processed_dataset.show_tokenization(i)\n",
        "    print(\"\\n\" + \".\"*60)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "JtCk7Lv55gGX"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
        "import numpy as np\n",
        "import torch\n",
        "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "import pandas as pd\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "mREjzFeoRc_s"
      },
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# FUNGSI COMPUTE METRICS (TANPA ACCURACY)\n",
        "# ===================================================================\n",
        "\n",
        "def compute_metrics(p):\n",
        "    \"\"\"\n",
        "    Fungsi untuk menghitung metrik: precision, recall, f1-score, roc-auc\n",
        "    \"\"\"\n",
        "    preds = p.predictions\n",
        "    labels = p.label_ids\n",
        "\n",
        "    # Get predicted classes\n",
        "    pred_classes = np.argmax(preds, axis=1);\n",
        "\n",
        "    # Precision, Recall, F1-Score\n",
        "    prec, rec, f1, _ = precision_recall_fscore_support(labels, pred_classes, average='weighted', zero_division=0)\n",
        "\n",
        "    # Coherence calculation (simplified as consistency metric)\n",
        "    # Coherence di sini dihitung sebagai konsistensi prediksi dengan confidence\n",
        "    max_probs = np.max(preds, axis=1)\n",
        "    coherence = np.mean(max_probs)  # Average confidence as coherence proxy\n",
        "\n",
        "    return {\n",
        "        'precision': prec,\n",
        "        'recall': rec,\n",
        "        'f1': f1,\n",
        "        'coherence': coherence\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 651
        },
        "id": "URNlpV2lR6Nk",
        "outputId": "ffb5bd24-34e8-4e89-a592-e9cd08dfccb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "======================================================================\n",
            "TRAINING MODEL TOPIC 1: PENGALAMAN UMUM PENGGUNAAN LINE\n",
            "======================================================================\n",
            "Jumlah label Topic 1: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai training Topic 1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jsonschema/_format.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 304\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mrfc3987\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    305\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'rfc3987'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lark/visitors.py\u001b[0m in \u001b[0;36m_call_userfunc\u001b[0;34m(self, tree, new_children)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: 'PrepareLiterals' object has no attribute 'expansion'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-849319517.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m# Training Topic 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Memulai training Topic 1...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m \u001b[0mtrainer_topic1\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[1;32m   2323\u001b[0m                 \u001b[0mhf_hub_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable_progress_bars\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2324\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2325\u001b[0;31m             return inner_training_loop(\n\u001b[0m\u001b[1;32m   2326\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2327\u001b[0m                 \u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresume_from_checkpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m_inner_training_loop\u001b[0;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[1;32m   2571\u001b[0m         \u001b[0mgrad_norm\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2572\u001b[0m         \u001b[0mlearning_rate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2573\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallback_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2574\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2575\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval_on_start\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control)\u001b[0m\n\u001b[1;32m    504\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    505\u001b[0m         \u001b[0mcontrol\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_training_stop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 506\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_train_begin\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    508\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerState\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainerControl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/trainer_callback.py\u001b[0m in \u001b[0;36mcall_event\u001b[0;34m(self, event, args, state, control, **kwargs)\u001b[0m\n\u001b[1;32m    554\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcall_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    555\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mcallback\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 556\u001b[0;31m             result = getattr(callback, event)(\n\u001b[0m\u001b[1;32m    557\u001b[0m                 \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    558\u001b[0m                 \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36mon_train_begin\u001b[0;34m(self, args, state, control, model, **kwargs)\u001b[0m\n\u001b[1;32m    964\u001b[0m             \u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialized\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 966\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    967\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    968\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mon_train_end\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontrol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocessing_class\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/integrations/integration_utils.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, args, state, model, **kwargs)\u001b[0m\n\u001b[1;32m    891\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 893\u001b[0;31m                 self._wandb.init(\n\u001b[0m\u001b[1;32m    894\u001b[0m                     \u001b[0mproject\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetenv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"WANDB_PROJECT\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"huggingface\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m                     \u001b[0;34m**\u001b[0m\u001b[0minit_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36minit\u001b[0;34m(entity, project, dir, id, name, notes, tags, config, config_exclude_keys, config_include_keys, allow_val_change, group, job_type, mode, force, reinit, resume, resume_from, fork_from, save_code, tensorboard, sync_tensorboard, monitor_gym, settings, anonymous)\u001b[0m\n\u001b[1;32m   1513\u001b[0m         \u001b[0mwi\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_WandbInit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_telemetry\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1514\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1515\u001b[0;31m         \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmaybe_login\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1516\u001b[0m         \u001b[0mrun_settings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshow_warnings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_run_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_init.py\u001b[0m in \u001b[0;36mmaybe_login\u001b[0;34m(self, init_settings)\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0;31m# Calling login() may change settings on the singleton,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m         \u001b[0;31m# so these may not be the final run settings.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 183\u001b[0;31m         \u001b[0mrun_settings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_wl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_copy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    184\u001b[0m         \u001b[0mrun_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_from_settings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minit_settings\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    185\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_setup.py\u001b[0m in \u001b[0;36msettings\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    305\u001b[0m         \"\"\"\n\u001b[1;32m    306\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 307\u001b[0;31m             self._load_settings(\n\u001b[0m\u001b[1;32m    308\u001b[0m                 \u001b[0msystem_settings_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m                 \u001b[0mdisable_sagemaker\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_setup.py\u001b[0m in \u001b[0;36m_load_settings\u001b[0;34m(self, system_settings_path, disable_sagemaker, overrides)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# infer settings from the system environment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_settings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_from_system_environment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# load SageMaker settings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/sdk/wandb_settings.py\u001b[0m in \u001b[0;36mupdate_from_system_environment\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1826\u001b[0m         \u001b[0;31m# Attempt to get notebook information if not already set by the user\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1827\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jupyter\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook_name\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook_name\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1828\u001b[0;31m             \u001b[0mmeta\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjupyter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnotebook_metadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msilent\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1829\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_jupyter_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"path\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1830\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mx_jupyter_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mnotebook_metadata\u001b[0;34m(silent)\u001b[0m\n\u001b[1;32m    228\u001b[0m     )\n\u001b[1;32m    229\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 230\u001b[0;31m         \u001b[0mjupyter_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnotebook_metadata_from_jupyter_servers_and_kernel_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    232\u001b[0m         \u001b[0;31m# Colab:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mnotebook_metadata_from_jupyter_servers_and_kernel_id\u001b[0;34m()\u001b[0m\n\u001b[1;32m    191\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 193\u001b[0;31m     \u001b[0mservers\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjupyter_servers_and_kernel_id\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    194\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mservers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"password\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/jupyter.py\u001b[0m in \u001b[0;36mjupyter_servers_and_kernel_id\u001b[0;34m()\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mservers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    277\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mserverapp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 278\u001b[0;31m             \u001b[0mservers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mserverapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_running_servers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    279\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnotebookapp\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    280\u001b[0m             \u001b[0mservers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnotebookapp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_running_servers\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/util.py\u001b[0m in \u001b[0;36m__getattribute__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    181\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"__lazy_module_state__\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 182\u001b[0;31m         \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    183\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    184\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/wandb/util.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    166\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__spec__\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__spec__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__spec__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexec_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mModuleType\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.12/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_call_with_frames_removed\u001b[0;34m(f, *args, **kwds)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jupyter_server/serverapp.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjupyter_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplication\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mJupyterApp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_aliases\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbase_flags\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjupyter_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpaths\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mjupyter_runtime_dir\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjupyter_events\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEventLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mnbformat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msign\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNotebookNotary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtornado\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhttpserver\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioloop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jupyter_events/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# flake8: noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_version\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mlogger\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEVENTS_METADATA_VERSION\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEventLogger\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mEventSchema\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jupyter_events/logger.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetadata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjsonschema\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpackaging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtraitlets\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jsonschema/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mjsonschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_format\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFormatChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjsonschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_types\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mTypeChecker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mjsonschema\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexceptions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSchemaError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValidationError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/jsonschema/_format.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    327\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0msuppress\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mImportError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 328\u001b[0;31m         \u001b[0;32mfrom\u001b[0m \u001b[0mrfc3987_syntax\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mis_valid_syntax\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_rfc3987_is_valid_syntax\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    329\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m         @_checks_drafts(\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/rfc3987_syntax/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msyntax_helpers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/rfc3987_syntax/syntax_helpers.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0mis_valid_syntax_ihost\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_syntax_validator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ihost\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m \u001b[0mis_valid_syntax_ireg_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_syntax_validator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ireg_name\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0mis_valid_syntax_ipath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmake_syntax_validator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"ipath\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/rfc3987_syntax/syntax_helpers.py\u001b[0m in \u001b[0;36mmake_syntax_validator\u001b[0;34m(rule_name)\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake_syntax_validator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLark\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparser\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mRFC3987_SYNTAX_PARSER_TYPE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msyntax_validator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lark/lark.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, grammar, **options)\u001b[0m\n\u001b[1;32m    422\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    423\u001b[0m         \u001b[0;31m# Compile the EBNF grammar into BNF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 424\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminals\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrules\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrammar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminals_to_keep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    425\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    426\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0medit_terminals\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lark/load_grammar.py\u001b[0m in \u001b[0;36mcompile\u001b[0;34m(self, start, terminals_to_keep)\u001b[0m\n\u001b[1;32m    738\u001b[0m             \u001b[0mebnf_to_bnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m             \u001b[0manon_tokens_transf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrule_options\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrule_options\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 740\u001b[0;31m             \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrule_tree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    741\u001b[0m             \u001b[0mres\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mebnf_to_bnf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    742\u001b[0m             \u001b[0mrules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lark/visitors.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, tree)\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_Leaf_T\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_Return_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransformers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_Return_T\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lark/visitors.py\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, tree)\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_Leaf_T\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_Return_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    283\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0msubtree\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter_subtrees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 284\u001b[0;31m             \u001b[0msubtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_children\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    285\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    286\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lark/visitors.py\u001b[0m in \u001b[0;36m_transform_children\u001b[0;34m(self, children)\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transform_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__visit_tokens__\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m                 \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_userfunc_token\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lark/visitors.py\u001b[0m in \u001b[0;36m_transform_tree\u001b[0;34m(self, tree)\u001b[0m\n\u001b[1;32m    278\u001b[0m     \"\"\"\n\u001b[1;32m    279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_transform_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m           \u001b[0;31m# Cancel recursion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 280\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_userfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    281\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    282\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0m_Leaf_T\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0m_Return_T\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lark/visitors.py\u001b[0m in \u001b[0;36m_call_userfunc\u001b[0;34m(self, tree, new_children)\u001b[0m\n\u001b[1;32m    115\u001b[0m             \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mAttributeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 117\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__default__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    118\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/lark/visitors.py\u001b[0m in \u001b[0;36m__default__\u001b[0;34m(self, data, children, meta)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0mCan\u001b[0m \u001b[0mbe\u001b[0m \u001b[0moverridden\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m \u001b[0mcreating\u001b[0m \u001b[0ma\u001b[0m \u001b[0mnew\u001b[0m \u001b[0mcopy\u001b[0m \u001b[0mof\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mtree\u001b[0m \u001b[0mnode\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mreturn\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m         \"\"\"\n\u001b[0;32m--> 180\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchildren\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmeta\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__default_token__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# TRAINING TOPIC 1 - PENGALAMAN UMUM PENGGUNAAN LINE\n",
        "# ===================================================================\n",
        "\n",
        "print(\"=\"*70)\n",
        "print(\"TRAINING MODEL TOPIC 1: PENGALAMAN UMUM PENGGUNAAN LINE\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Hitung jumlah label unik untuk Topic 1\n",
        "labels_t1 = [train_dataset_t1[i]['labels'].item() for i in range(len(train_dataset_t1))]\n",
        "num_labels_t1 = len(set(labels_t1))\n",
        "print(f\"Jumlah label Topic 1: {num_labels_t1}\")\n",
        "\n",
        "# Load model untuk Topic 1\n",
        "model_topic1 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"indobenchmark/indobert-base-p1\", # Changed 'model' to the model identifier string\n",
        "    num_labels=num_labels_t1,\n",
        "    problem_type=\"single_label_classification\"\n",
        ")\n",
        "model_topic1.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training arguments untuk Topic 1\n",
        "training_args_topic1 = TrainingArguments(\n",
        "    output_dir=\"/content/indobert_topic1_model\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=3e-5,\n",
        "    warmup_steps=100,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"/content/logs_topic1\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "# Trainer untuk Topic 1\n",
        "trainer_topic1 = Trainer(\n",
        "    model=model_topic1,\n",
        "    args=training_args_topic1,\n",
        "    train_dataset=train_dataset_t1,\n",
        "    eval_dataset=val_dataset_t1,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Training Topic 1\n",
        "print(\"Memulai training Topic 1...\")\n",
        "trainer_topic1.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 139
        },
        "id": "79gED2n0gUZM",
        "outputId": "8d7e6d14-c9f1-46c4-9313-9041c72117e9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EVALUASI TOPIC 1 ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasil Evaluasi Topic 1:\n",
            "   split  precision  recall      f1  coherence\n",
            "0  train     0.9818  0.9817  0.9818     4.3135\n",
            "1   test     0.8767  0.8751  0.8757     4.0244\n",
            "Model Topic 1 disimpan di: /content/indobert_topic1_model_final\n"
          ]
        }
      ],
      "source": [
        "# Evaluasi Topic 1\n",
        "print(\"\\n=== EVALUASI TOPIC 1 ===\")\n",
        "# 1. Evaluate on training set\n",
        "train_eval_t1 = trainer_topic1.evaluate(train_dataset_t1)\n",
        "# 2. Evaluate on test set\n",
        "test_pred_t1 = trainer_topic1.predict(test_dataset_t1)\n",
        "test_eval_t1 = compute_metrics(test_pred_t1)\n",
        "\n",
        "# 3. Hasil metrik Topic 1\n",
        "df_metrics_t1 = pd.DataFrame([\n",
        "    {\n",
        "        'split': 'train',\n",
        "        'precision': train_eval_t1['eval_precision'],\n",
        "        'recall': train_eval_t1['eval_recall'],\n",
        "        'f1': train_eval_t1['eval_f1'],\n",
        "        'coherence': train_eval_t1['eval_coherence']\n",
        "    },\n",
        "    {\n",
        "        'split': 'test',\n",
        "        'precision': test_eval_t1['precision'],\n",
        "        'recall': test_eval_t1['recall'],\n",
        "        'f1': test_eval_t1['f1'],\n",
        "        'coherence': test_eval_t1['coherence']\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"Hasil Evaluasi Topic 1:\")\n",
        "print(df_metrics_t1.round(4))\n",
        "\n",
        "# Simpan model Topic 1\n",
        "OUTPUT_DIR_TOPIC1 = \"/content/indobert_topic1_model_final\"\n",
        "model_topic1.save_pretrained(OUTPUT_DIR_TOPIC1)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR_TOPIC1)\n",
        "print(f\"Model Topic 1 disimpan di: {OUTPUT_DIR_TOPIC1}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "5e1zru_EhpZD",
        "outputId": "e6df243a-121e-4fa8-d9b0-2505e3f95a80"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TRAINING MODEL TOPIC 2: FITUR TAMBAHAN\n",
            "======================================================================\n",
            "Jumlah label Topic 2: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai training Topic 2...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='699' max='699' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [699/699 06:28, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Coherence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.113175</td>\n",
              "      <td>0.962335</td>\n",
              "      <td>0.966165</td>\n",
              "      <td>0.964190</td>\n",
              "      <td>3.514601</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.103529</td>\n",
              "      <td>0.962319</td>\n",
              "      <td>0.966165</td>\n",
              "      <td>0.964097</td>\n",
              "      <td>3.889197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.272900</td>\n",
              "      <td>0.105301</td>\n",
              "      <td>0.968514</td>\n",
              "      <td>0.971805</td>\n",
              "      <td>0.970099</td>\n",
              "      <td>4.904456</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== EVALUASI TOPIC 2 ===\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": []
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hasil Evaluasi Topic 2:\n",
            "   split  precision  recall      f1  coherence\n",
            "0  train     0.9869  0.9903  0.9886     4.9901\n",
            "1   test     0.9561  0.9587  0.9572     4.8860\n",
            "Model Topic 2 disimpan di: /content/indobert_topic2_model_final\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING MODEL TOPIC 2: FITUR TAMBAHAN\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Hitung jumlah label unik untuk Topic 2\n",
        "labels_t2 = [train_dataset_t2[i]['labels'].item() for i in range(len(train_dataset_t2))]\n",
        "num_labels_t2 = len(set(labels_t2))\n",
        "print(f\"Jumlah label Topic 2: {num_labels_t2}\")\n",
        "\n",
        "# Load model untuk Topic 2\n",
        "model_topic2 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"indobenchmark/indobert-base-p1\", # Changed 'MODEL_NAME' to the model identifier string\n",
        "    num_labels=num_labels_t2,\n",
        "    problem_type=\"single_label_classification\"\n",
        ")\n",
        "model_topic2.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training arguments untuk Topic 2\n",
        "training_args_topic2 = TrainingArguments(\n",
        "    output_dir=\"/content/indobert_topic2_model\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=2e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"/content/logs_topic2\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "# Trainer untuk Topic 2\n",
        "trainer_topic2 = Trainer(\n",
        "    model=model_topic2,\n",
        "    args=training_args_topic2,\n",
        "    train_dataset=train_dataset_t2,\n",
        "    eval_dataset=val_dataset_t2,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Training Topic 2\n",
        "print(\"Memulai training Topic 2...\")\n",
        "trainer_topic2.train()\n",
        "\n",
        "# Evaluasi Topic 2\n",
        "print(\"\\n=== EVALUASI TOPIC 2 ===\")\n",
        "# 1. Evaluate on training set\n",
        "train_eval_t2 = trainer_topic2.evaluate(train_dataset_t2)\n",
        "# 2. Evaluate on test set\n",
        "test_pred_t2 = trainer_topic2.predict(test_dataset_t2)\n",
        "test_eval_t2 = compute_metrics(test_pred_t2)\n",
        "\n",
        "# 3. Hasil metrik Topic 2\n",
        "df_metrics_t2 = pd.DataFrame([\n",
        "    {\n",
        "        'split': 'train',\n",
        "        'precision': train_eval_t2['eval_precision'],\n",
        "        'recall': train_eval_t2['eval_recall'],\n",
        "        'f1': train_eval_t2['eval_f1'],\n",
        "        'coherence': train_eval_t2['eval_coherence']\n",
        "    },\n",
        "    {\n",
        "        'split': 'test',\n",
        "        'precision': test_eval_t2['precision'],\n",
        "        'recall': test_eval_t2['recall'],\n",
        "        'f1': test_eval_t2['f1'],\n",
        "        'coherence': test_eval_t2['coherence']\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"Hasil Evaluasi Topic 2:\")\n",
        "print(df_metrics_t2.round(4))\n",
        "\n",
        "# Simpan model Topic 2\n",
        "OUTPUT_DIR_TOPIC2 = \"/content/indobert_topic2_model_final\"\n",
        "model_topic2.save_pretrained(OUTPUT_DIR_TOPIC2)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR_TOPIC2)\n",
        "print(f\"Model Topic 2 disimpan di: {OUTPUT_DIR_TOPIC2}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "91Iv4Juti9Dt",
        "outputId": "977cad37-c067-440f-d0d9-36d0e4a09254"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======================================================================\n",
            "TRAINING MODEL TOPIC 3: LOGIN DAN REGISTRASI AKUN\n",
            "======================================================================\n",
            "Jumlah label Topic 3: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at indobenchmark/indobert-base-p1 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memulai training Topic 3...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='271' max='699' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [271/699 02:20 < 03:43, 1.91 it/s, Epoch 1.16/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Precision</th>\n",
              "      <th>Recall</th>\n",
              "      <th>F1</th>\n",
              "      <th>Coherence</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>No log</td>\n",
              "      <td>0.221514</td>\n",
              "      <td>0.920998</td>\n",
              "      <td>0.922932</td>\n",
              "      <td>0.921925</td>\n",
              "      <td>3.552344</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "  # ===================================================================\n",
        "# TRAINING TOPIC 3 - LOGIN DAN REGISTRASI AKUN\n",
        "# ===================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"TRAINING MODEL TOPIC 3: LOGIN DAN REGISTRASI AKUN\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "# Hitung jumlah label unik untuk Topic 3\n",
        "labels_t3 = [train_dataset_t3[i]['labels'].item() for i in range(len(train_dataset_t3))]\n",
        "num_labels_t3 = len(set(labels_t3))\n",
        "print(f\"Jumlah label Topic 3: {num_labels_t3}\")\n",
        "\n",
        "# Load model untuk Topic 3\n",
        "model_topic3 = AutoModelForSequenceClassification.from_pretrained(\n",
        "    \"indobenchmark/indobert-base-p1\", # Changed 'MODEL_NAME' to the model identifier string\n",
        "    num_labels=num_labels_t3,\n",
        "    problem_type=\"single_label_classification\"\n",
        ")\n",
        "model_topic3.to(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Training arguments untuk Topic 3\n",
        "training_args_topic3 = TrainingArguments(\n",
        "    output_dir=\"/content/indobert_topic3_model\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=16,\n",
        "    per_device_eval_batch_size=32,\n",
        "    learning_rate=5e-5,\n",
        "    warmup_steps=500,\n",
        "    weight_decay=0.01,\n",
        "    eval_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_dir=\"/content/logs_topic3\",\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"f1\",\n",
        "    greater_is_better=True,\n",
        "    report_to=None\n",
        ")\n",
        "\n",
        "# Trainer untuk Topic 3\n",
        "trainer_topic3 = Trainer(\n",
        "    model=model_topic3,\n",
        "    args=training_args_topic3,\n",
        "    train_dataset=train_dataset_t3,\n",
        "    eval_dataset=val_dataset_t3,\n",
        "    compute_metrics=compute_metrics\n",
        ")\n",
        "\n",
        "# Training Topic 3\n",
        "print(\"Memulai training Topic 3...\")\n",
        "trainer_topic3.train()\n",
        "\n",
        "# Evaluasi Topic 3\n",
        "print(\"\\n=== EVALUASI TOPIC 3 ===\")\n",
        "# 1. Evaluate on training set\n",
        "train_eval_t3 = trainer_topic3.evaluate(train_dataset_t3)\n",
        "# 2. Evaluate on test set\n",
        "test_pred_t3 = trainer_topic3.predict(test_dataset_t3)\n",
        "test_eval_t3 = compute_metrics(test_pred_t3)\n",
        "\n",
        "# 3. Hasil metrik Topic 3\n",
        "df_metrics_t3 = pd.DataFrame([\n",
        "    {\n",
        "        'split': 'train',\n",
        "        'precision': train_eval_t3['eval_precision'],\n",
        "        'recall': train_eval_t3['eval_recall'],\n",
        "        'f1': train_eval_t3['eval_f1'],\n",
        "        'coherence': train_eval_t3['eval_coherence']\n",
        "    },\n",
        "    {\n",
        "        'split': 'test',\n",
        "        'precision': test_eval_t3['precision'],\n",
        "        'recall': test_eval_t3['recall'],\n",
        "        'f1': test_eval_t3['f1'],\n",
        "        'coherence': test_eval_t3['coherence']\n",
        "    }\n",
        "])\n",
        "\n",
        "print(\"Hasil Evaluasi Topic 3:\")\n",
        "print(df_metrics_t3.round(4))\n",
        "\n",
        "# Simpan model Topic 3\n",
        "OUTPUT_DIR_TOPIC3 = \"/content/indobert_topic3_model_final\"\n",
        "model_topic3.save_pretrained(OUTPUT_DIR_TOPIC3)\n",
        "tokenizer.save_pretrained(OUTPUT_DIR_TOPIC3)\n",
        "print(f\"Model Topic 3 disimpan di: {OUTPUT_DIR_TOPIC3}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}